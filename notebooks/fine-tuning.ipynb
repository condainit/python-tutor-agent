{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d2ebbd2-5dc6-4547-b9ae-2bd56619edf6",
    "_uuid": "55a1ceb1-1aa0-498d-8288-9f6fe7e7743d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# LoRA Fine-tuning\n",
    "\n",
    "This notebook fine-tunes an instruction-tuned base model (Qwen2.5-3B-Instruct) with LoRA to generate short Python tutoring hints from MBPP-style data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "71ef3047-30af-4e52-a845-404e76e79c07",
    "_uuid": "7c439fd8-077d-4086-bd31-a1bbb57495ef",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Prerequisites\n",
    "1. Run `python -m data.preprocess` locally to create the `processed/` folder.  \n",
    "2. Zip the `processed/` folder and upload it as a private Kaggle Dataset (e.g., “Python Tutor Agent Dataset”).  \n",
    "   On Kaggle, it will mount at:\n",
    "\n",
    "   ```\n",
    "   /kaggle/input/python-tutor-agent-dataset/processed\n",
    "   ```\n",
    "\n",
    "## Quick Start (Kaggle)\n",
    "1. Create a new Kaggle Notebook, click Add data, and select your private dataset (e.g., `python-tutor-agent-dataset`).\n",
    "2. Settings  \n",
    "   - Accelerator: GPU (T4/V100/A100)  \n",
    "   - Internet: On (recommended for first run to fetch base model)  \n",
    "   - Session persistence: Files only or Both if you want `/kaggle/working/outputs` to survive restarts\n",
    "3. Edit the first code cell:\n",
    "   - `DATASET_NAME = \"python-tutor-agent-dataset\"`\n",
    "   - `BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"`\n",
    "   - `DATA_ROOT = f\"/kaggle/input/{DATASET_NAME}/processed\"`\n",
    "4. Run all cells (top to bottom)\n",
    "   - Outputs (adapter + logs) are written to:  \n",
    "   `/kaggle/working/outputs/<base>-hints`  \n",
    "   - TensorBoard logs are under:  \n",
    "   `/kaggle/working/outputs/<base>-hints/tb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23ebb626-2ab3-4592-8d16-92a3e084f0c0",
    "_uuid": "bcee08cc-7647-49d4-bc31-1ed712ecc988",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc5a465b-4a31-44af-a65f-66570c74b1fe",
    "_uuid": "6a31075e-6cef-40e6-b567-04cd8e4144e6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:24:17.826050Z",
     "iopub.status.busy": "2025-10-12T00:24:17.825787Z",
     "iopub.status.idle": "2025-10-12T00:26:29.507421Z",
     "shell.execute_reply": "2025-10-12T00:26:29.506609Z",
     "shell.execute_reply.started": "2025-10-12T00:24:17.826030Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -q --no-deps \"trl==0.9.6\"\n",
    "%pip install -q --upgrade \"transformers==4.45.2\" \"accelerate==1.2.1\" \"peft==0.13.2\" \"huggingface_hub==0.35.3\" \"tyro>=0.5.11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a06c8ace-cc79-456c-aed2-6e76d8865e06",
    "_uuid": "0a5c3421-23d4-4f40-bd92-4ab3e4b788d1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62ca0d26-592f-4159-9dfc-a77b0ae8ea4f",
    "_uuid": "a57e84dd-9413-4a96-89b8-1c311d8940b2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:26:29.509084Z",
     "iopub.status.busy": "2025-10-12T00:26:29.508883Z",
     "iopub.status.idle": "2025-10-12T00:27:08.620421Z",
     "shell.execute_reply": "2025-10-12T00:27:08.619826Z",
     "shell.execute_reply.started": "2025-10-12T00:26:29.509067Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json, os, platform, sys, math, shutil, tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import random\n",
    "from random import sample\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f2fc66bc-423f-42df-8b0a-35a44e573d47",
    "_uuid": "4e0542e5-4363-4983-9a6f-040f283fbb72",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Configure dataset, base model, and LoRA hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef5eee35-54bf-4fae-8211-1eeee2191209",
    "_uuid": "e8aa07d5-1df6-4019-92cc-819be47a884d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:08.621604Z",
     "iopub.status.busy": "2025-10-12T00:27:08.621105Z",
     "iopub.status.idle": "2025-10-12T00:27:08.636775Z",
     "shell.execute_reply": "2025-10-12T00:27:08.635987Z",
     "shell.execute_reply.started": "2025-10-12T00:27:08.621575Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"python-tutor-agent-dataset\"\n",
    "BASE_MODEL   = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# LoRA training knobs\n",
    "BATCH_SIZE   = 1\n",
    "GRAD_ACCUM   = 8\n",
    "EPOCHS       = 5\n",
    "LR           = 1e-4\n",
    "MAX_LEN      = 2048\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# LoRA adapter config\n",
    "LORA_R       = 16\n",
    "LORA_ALPHA   = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Reproducibility seed\n",
    "SEED         = 42\n",
    "\n",
    "# Derived path\n",
    "DATA_ROOT = f\"/kaggle/input/{DATASET_NAME}/processed\"\n",
    "\n",
    "# Precision auto-detect (bf16 on Ampere+; otherwise fp16)\n",
    "try:\n",
    "    cap = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0, 0)\n",
    "    USE_BF16 = torch.cuda.is_available() and cap[0] >= 8\n",
    "except Exception:\n",
    "    USE_BF16 = False\n",
    "\n",
    "print(\"USE_BF16:\", USE_BF16)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"BASE_MODEL:\", BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54d4a8e9-1d3e-4771-a4f9-6471a2c4895a",
    "_uuid": "1f74e9ec-dc3a-4a45-974b-787cd753670b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Define tutoring prompts and enforce output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7544bd8a-0042-4576-a48d-390470828841",
    "_uuid": "19311c78-d8e6-4340-9285-a7cdf3fa68bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:08.637869Z",
     "iopub.status.busy": "2025-10-12T00:27:08.637606Z",
     "iopub.status.idle": "2025-10-12T00:27:08.651710Z",
     "shell.execute_reply": "2025-10-12T00:27:08.650967Z",
     "shell.execute_reply.started": "2025-10-12T00:27:08.637850Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HINT_PROMPT = \"\"\"Problem:\n",
    "{problem}\n",
    "\n",
    "Learner code:\n",
    "{learner_code}\n",
    "\n",
    "Failed unit tests:\n",
    "```json\n",
    "{failed_tests_json}\n",
    "```\n",
    "\n",
    "Write one short, actionable hint (1–2 sentences). No code. Do not quote the tests verbatim.\n",
    "Hint: \"\"\"\n",
    "\n",
    "RESPONSE_PREFIX = \"Hint: \"\n",
    "\n",
    "assert HINT_PROMPT.endswith(RESPONSE_PREFIX), \"HINT_PROMPT must end with 'Hint: '\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85e3fa1b-6e50-4f4d-8e64-de2c7f80ee98",
    "_uuid": "92613d71-d5ef-4898-af8c-f093ddcd44f3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Data helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b050d61-eeaa-41ff-a173-e37f8ee044fa",
    "_uuid": "278ac174-9c03-44ce-8281-41aac7d80128",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:08.653861Z",
     "iopub.status.busy": "2025-10-12T00:27:08.653653Z",
     "iopub.status.idle": "2025-10-12T00:27:08.672349Z",
     "shell.execute_reply": "2025-10-12T00:27:08.671617Z",
     "shell.execute_reply.started": "2025-10-12T00:27:08.653846Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _read_jsonl(path: Path) -> List[Dict]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def _read_text(path: Path) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "def _resolve_failed_tests_json(data_root_path: Path, failed_tests_path: str) -> str:\n",
    "    p = (data_root_path / failed_tests_path).resolve()\n",
    "    return _read_text(p)\n",
    "\n",
    "def _build_prompt(problem: str, learner_code: str, failed_tests_json: str) -> str:\n",
    "    return HINT_PROMPT.format(\n",
    "        problem=problem,\n",
    "        learner_code=learner_code,\n",
    "        failed_tests_json=failed_tests_json,\n",
    "    )\n",
    "\n",
    "def make_sft_dataset(jsonl_path: Path, data_root_path: Path) -> Dataset:\n",
    "    rows = _read_jsonl(jsonl_path)\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        prompt  = _build_prompt(\n",
    "            r[\"problem\"],\n",
    "            r[\"learner_code\"],\n",
    "            _resolve_failed_tests_json(data_root_path, r[\"failed_tests_path\"]),\n",
    "        )\n",
    "        text = prompt + (r.get(\"hint\") or \"\").strip()\n",
    "        records.append({\"text\": text})\n",
    "    return Dataset.from_list(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f14fe978-a76c-4cde-90d7-c619bd69810a",
    "_uuid": "1ce2e3e0-3aa5-47c1-b226-f80409a02f52",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## LoRA helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "772f05ef-94ac-4f2f-aa79-8260cfdefb8f",
    "_uuid": "b114b9ca-7976-4015-969d-46d19b7357fd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:08.673394Z",
     "iopub.status.busy": "2025-10-12T00:27:08.673132Z",
     "iopub.status.idle": "2025-10-12T00:27:08.692292Z",
     "shell.execute_reply": "2025-10-12T00:27:08.691773Z",
     "shell.execute_reply.started": "2025-10-12T00:27:08.673361Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pick_lora_targets():\n",
    "    return [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "def wrap_with_lora(model, target_modules, r, alpha, dropout, base_model_name_or_path=\"\"):\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r, lora_alpha=alpha, lora_dropout=dropout, bias=\"none\",\n",
    "        target_modules=target_modules, task_type=\"CAUSAL_LM\",\n",
    "        base_model_name_or_path=base_model_name_or_path or None,\n",
    "    )\n",
    "    return get_peft_model(model, lora_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c788162-72e7-4c9a-90b8-aa9b32bf35d9",
    "_uuid": "aea69c27-8913-4e32-8cba-9cb4ef669c97",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7708e8e5-9039-4714-8f3b-bb7d03cbf6c9",
    "_uuid": "81337ac3-5162-41d3-bb79-14609e07be8d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:08.693141Z",
     "iopub.status.busy": "2025-10-12T00:27:08.692917Z",
     "iopub.status.idle": "2025-10-12T00:27:09.642184Z",
     "shell.execute_reply": "2025-10-12T00:27:09.641350Z",
     "shell.execute_reply.started": "2025-10-12T00:27:08.693125Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_file = Path(DATA_ROOT) / \"lora_train.jsonl\"\n",
    "val_file  = Path(DATA_ROOT) / \"lora_val.jsonl\"\n",
    "data_root_path  = Path(DATA_ROOT)\n",
    "\n",
    "train_ds = make_sft_dataset(train_file, data_root_path)\n",
    "val_ds  = make_sft_dataset(val_file,  data_root_path)\n",
    "print(\"Rows -> train:\", len(train_ds), \"eval:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6277eaf-9a3d-428c-9b3e-034af03fecbe",
    "_uuid": "c3057225-32fc-41f0-8e3f-0edcac8a23ad",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Load model/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e459ebdd-f9af-42d9-bf9d-325b1b3f3232",
    "_uuid": "d9d52816-fa9c-4b3f-b77c-4ee9dfc3d80b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:27:09.643181Z",
     "iopub.status.busy": "2025-10-12T00:27:09.642931Z",
     "iopub.status.idle": "2025-10-12T00:28:00.269659Z",
     "shell.execute_reply": "2025-10-12T00:28:00.268780Z",
     "shell.execute_reply.started": "2025-10-12T00:27:09.643163Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id: str, bf16: bool, max_len: int = 2048):\n",
    "    dtype = torch.bfloat16 if bf16 else torch.float16\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    try: tok.chat_template = None\n",
    "    except Exception: pass\n",
    "\n",
    "    if tok.pad_token is None and tok.eos_token is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"right\"\n",
    "    tok.model_max_length = max_len\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    model.config.eos_token_id = tok.eos_token_id\n",
    "\n",
    "    print(f\"Loaded {model_id} dtype={dtype} pad={tok.pad_token_id} eos={tok.eos_token_id}\")\n",
    "    return model, tok\n",
    "\n",
    "model, tok = load_model_and_tokenizer(BASE_MODEL, USE_BF16, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29b2b5da-7cde-483f-b354-dee69723305a",
    "_uuid": "d406d9a9-1a0a-4acf-876d-a6bd7f6be2e4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6bdeeea6-fb90-4df0-b7d0-61b7b18bbefb",
    "_uuid": "33f81264-684f-4b46-aa32-fda3de819f25",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:28:00.270660Z",
     "iopub.status.busy": "2025-10-12T00:28:00.270409Z",
     "iopub.status.idle": "2025-10-12T00:28:05.633730Z",
     "shell.execute_reply": "2025-10-12T00:28:05.632912Z",
     "shell.execute_reply.started": "2025-10-12T00:28:00.270643Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _tok_map(batch):\n",
    "    enc = tok(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=False,\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "train_tok = train_ds.map(_tok_map, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(_tok_map,   batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7578e9a-c4cc-4f3e-82b4-9157da6abbe1",
    "_uuid": "a72083ea-1545-48ce-bb77-6e5350a80ffc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Apply LoRA adapter and enable input grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14a6c72d-9f42-46b1-a38e-8ced7ffa44e6",
    "_uuid": "5a95df47-cf21-4db0-b6fc-ee0d32a259b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:28:05.634813Z",
     "iopub.status.busy": "2025-10-12T00:28:05.634488Z",
     "iopub.status.idle": "2025-10-12T00:28:11.881907Z",
     "shell.execute_reply": "2025-10-12T00:28:11.881084Z",
     "shell.execute_reply.started": "2025-10-12T00:28:05.634790Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = wrap_with_lora(\n",
    "    model,\n",
    "    target_modules=pick_lora_targets(),\n",
    "    r=LORA_R, alpha=LORA_ALPHA, dropout=LORA_DROPOUT,\n",
    "    base_model_name_or_path=BASE_MODEL,\n",
    ")\n",
    "print(f\"LoRA wrapped with r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")\n",
    "\n",
    "try:\n",
    "    model.enable_input_require_grads()\n",
    "except Exception as e:\n",
    "    print(\"enable_input_require_grads() not available:\", e)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d54b9f75-f260-4670-8bb8-79baf446eacf",
    "_uuid": "1bad2869-8564-4aea-b060-7590b9f27ab6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## TensorBoard (save logs for download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a8f91491-d1ce-45df-94fc-82df752b53f0",
    "_uuid": "20cdfc3f-f11a-4154-8b25-bfd70defe377",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:28:11.883149Z",
     "iopub.status.busy": "2025-10-12T00:28:11.882789Z",
     "iopub.status.idle": "2025-10-12T00:28:11.891358Z",
     "shell.execute_reply": "2025-10-12T00:28:11.890608Z",
     "shell.execute_reply.started": "2025-10-12T00:28:11.883125Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outdir = Path(\"/kaggle/working\") / f\"outputs/{BASE_MODEL.split('/')[-1].lower()}-hints\"\n",
    "tb_dir = outdir / \"tb\"\n",
    "tb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(str(tb_dir))\n",
    "writer.add_scalar(\"boot/started\", 1, 0)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "event_files = [p.name for p in tb_dir.glob(\"events.*\")]\n",
    "print(f\"TensorBoard logdir: {tb_dir}\")\n",
    "print(f\"Event files: {event_files if event_files else 'none yet (will be created during training)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0bc82d79-e89e-44a5-99f2-2dcba61f94c6",
    "_uuid": "a20a85a8-753e-48b7-ac55-b16fec52a724",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "871c0f58-1746-408d-9f45-b730a23cd67f",
    "_uuid": "f8fa83f5-a64f-4b57-a753-63bffda89f59",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:28:11.892449Z",
     "iopub.status.busy": "2025-10-12T00:28:11.892230Z",
     "iopub.status.idle": "2025-10-12T00:32:59.113838Z",
     "shell.execute_reply": "2025-10-12T00:32:59.112932Z",
     "shell.execute_reply.started": "2025-10-12T00:28:11.892433Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cfg = SFTConfig(\n",
    "    output_dir=str(outdir),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=max(1, BATCH_SIZE // 2),\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    logging_steps=25,\n",
    "    logging_dir=str(tb_dir),\n",
    "    logging_first_step=True,\n",
    "    run_name=f\"run-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}\",\n",
    "\n",
    "    # Evaluation & checkpointing\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # Precision & memory\n",
    "    bf16=USE_BF16,\n",
    "    fp16=(not USE_BF16),\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    # Data / trainer plumbing\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    max_seq_length=MAX_LEN,\n",
    "    packing=False,\n",
    "\n",
    "    # Niceties\n",
    "    save_safetensors=True,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    args=cfg,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4d40247-dd7b-4237-9a32-53c1dda379ea",
    "_uuid": "22676f2b-bc30-4bb6-af61-7bac5ef8f378",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Validation metrics (loss & perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdcdf186-bd5b-4025-bfea-147f1f50627b",
    "_uuid": "6446e89e-51f1-4aa9-95ab-8387a36fcd2f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:32:59.115180Z",
     "iopub.status.busy": "2025-10-12T00:32:59.114859Z",
     "iopub.status.idle": "2025-10-12T00:33:01.124917Z",
     "shell.execute_reply": "2025-10-12T00:33:01.123963Z",
     "shell.execute_reply.started": "2025-10-12T00:32:59.115153Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_metrics = trainer.evaluate(eval_dataset=val_tok)\n",
    "val_loss = float(val_metrics.get(\"eval_loss\", float(\"nan\")))\n",
    "val_ppl  = math.exp(val_loss) if math.isfinite(val_loss) else float(\"nan\")\n",
    "val_metrics[\"eval_ppl\"] = val_ppl\n",
    "print(json.dumps(val_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0056112-03e6-4393-aaa0-cebd297fed0c",
    "_uuid": "215c96b2-9545-4b50-9a0e-3def5a7737ed",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Qualitative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "64e06174-a50e-4f7f-9a8e-f55ac4ad244a",
    "_uuid": "ccbfca27-2cb5-4a54-b179-6606d7669553",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:33:01.128312Z",
     "iopub.status.busy": "2025-10-12T00:33:01.128061Z",
     "iopub.status.idle": "2025-10-12T00:33:10.909144Z",
     "shell.execute_reply": "2025-10-12T00:33:10.908286Z",
     "shell.execute_reply.started": "2025-10-12T00:33:01.128293Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for rec in sample(list(val_ds), 5):\n",
    "    prompt = rec[\"text\"].rsplit(\"Hint: \", 1)[0] + \"Hint: \"\n",
    "    toks = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**toks, max_new_tokens=64, do_sample=False)\n",
    "    print(tok.decode(out[0], skip_special_tokens=True).split(\"Hint: \",1)[-1].strip(), \"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d34e2e81-6499-4e07-a7ee-98cbcd06a965",
    "_uuid": "01964d7d-0a1f-43c0-9f7e-b6b5579f90d3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Save adapter & manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa096d4a-732f-4590-ac2a-b7e433575fc0",
    "_uuid": "5d2d5c99-2227-47c7-85b1-d5abc2f3db43",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:33:10.910329Z",
     "iopub.status.busy": "2025-10-12T00:33:10.910051Z",
     "iopub.status.idle": "2025-10-12T00:33:11.865016Z",
     "shell.execute_reply": "2025-10-12T00:33:11.864208Z",
     "shell.execute_reply.started": "2025-10-12T00:33:10.910304Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(outdir)\n",
    "tok.save_pretrained(outdir)\n",
    "trainer.save_state()\n",
    "\n",
    "paths = {\n",
    "    \"adapter_config\": outdir / \"adapter_config.json\",\n",
    "    \"hf_model_config\": outdir / \"config.json\",\n",
    "    \"tokenizer_config\": outdir / \"tokenizer_config.json\",\n",
    "    \"trainer_state\": outdir / \"trainer_state.json\",\n",
    "}\n",
    "\n",
    "def _safe_json(p: Path):\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_eval_ds = val_tok if \"val_tok\" in globals() else val_ds\n",
    "\n",
    "if \"val_metrics\" not in globals():\n",
    "    try:\n",
    "        _vm = trainer.evaluate(eval_dataset=_eval_ds)\n",
    "        if \"eval_loss\" in _vm and _vm[\"eval_loss\"] is not None:\n",
    "            _vm[\"eval_ppl\"] = float(math.exp(float(_vm[\"eval_loss\"])))\n",
    "        val_metrics = _vm\n",
    "    except Exception:\n",
    "        val_metrics = None\n",
    "\n",
    "best_ckpt = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
    "run_name  = getattr(trainer.args, \"run_name\", None)\n",
    "tb_dir    = str(getattr(trainer.args, \"logging_dir\", outdir / \"tb\"))\n",
    "\n",
    "try:\n",
    "    n_total = int(sum(p.numel() for p in trainer.model.parameters()))\n",
    "    n_train = int(sum(p.numel() for p in trainer.model.parameters() if p.requires_grad))\n",
    "    pct_train = 100.0 * n_train / max(1, n_total)\n",
    "except Exception:\n",
    "    n_total = n_train = 0\n",
    "    pct_train = 0.0\n",
    "\n",
    "state = trainer.state\n",
    "train_summary = {\n",
    "    \"global_step\": int(getattr(state, \"global_step\", 0) or 0),\n",
    "    \"epoch\": float(getattr(state, \"epoch\", 0.0) or 0.0),\n",
    "    \"train_samples\": int(len(train_ds)),\n",
    "    \"eval_samples\": int(len(val_ds)),\n",
    "}\n",
    "\n",
    "log_history_path = outdir / \"log_history.jsonl\"\n",
    "try:\n",
    "    with log_history_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in (state.log_history or []):\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "manifest = {\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"base_model\": BASE_MODEL,\n",
    "    \"qlora\": False,\n",
    "    \"precision\": {\n",
    "        \"use_bf16\": bool(USE_BF16),\n",
    "        \"torch_dtype\": str(next(trainer.model.parameters()).dtype),\n",
    "        \"device\": str(next(trainer.model.parameters()).device),\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"total\": n_total,\n",
    "        \"trainable\": n_train,\n",
    "        \"trainable_pct\": round(pct_train, 4),\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"train_file\": str(train_file),\n",
    "        \"eval_file\":  str(val_file),\n",
    "        \"data_root_path\": str(data_root_path),\n",
    "        \"num_train_rows\": int(len(train_ds)),\n",
    "        \"num_eval_rows\":  int(len(val_ds)),\n",
    "        \"max_seq_length\": MAX_LEN,\n",
    "    },\n",
    "    \"training_arguments\": {\n",
    "        \"output_dir\": str(outdir),\n",
    "        \"per_device_train_batch_size\": BATCH_SIZE,\n",
    "        \"per_device_eval_batch_size\": max(1, BATCH_SIZE // 2),\n",
    "        \"gradient_accumulation_steps\": GRAD_ACCUM,\n",
    "        \"num_train_epochs\": EPOCHS,\n",
    "        \"learning_rate\": LR,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"bf16\": bool(USE_BF16),\n",
    "        \"fp16\": (not USE_BF16),\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"seed\": SEED,\n",
    "        \"response_template\": RESPONSE_PREFIX,\n",
    "        \"run_name\": run_name,\n",
    "        \"tensorboard_logdir\": tb_dir,\n",
    "        \"best_model_checkpoint\": best_ckpt,\n",
    "    },\n",
    "    \"trainer_state\": train_summary,\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": pick_lora_targets(),\n",
    "        \"base_model_name_or_path\": BASE_MODEL,\n",
    "    },\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"artifacts\": {\n",
    "        \"output_dir\": str(outdir),\n",
    "        **{k: str(v) for k, v in paths.items()},\n",
    "        \"log_history\": str(log_history_path),\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"python\": platform.python_version(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"transformers\": __import__(\"transformers\").__version__,\n",
    "        \"peft\": __import__(\"peft\").__version__,\n",
    "        \"trl\": __import__(\"trl\").__version__,\n",
    "        \"cuda_visible_devices\": os.environ.get(\"CUDA_VISIBLE_DEVICES\"),\n",
    "        \"platform\": sys.platform,\n",
    "    },\n",
    "    \"snapshots\": {\n",
    "        \"adapter_config\": _safe_json(paths[\"adapter_config\"]),\n",
    "        \"hf_model_config\": _safe_json(paths[\"hf_model_config\"]),\n",
    "        \"tokenizer_config\": _safe_json(paths[\"tokenizer_config\"]),\n",
    "        \"trainer_state\": _safe_json(paths[\"trainer_state\"]),\n",
    "    },\n",
    "}\n",
    "\n",
    "(outdir / \"manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Saved LoRA adapter and manifest to {outdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "36dd29aa-f7f1-420a-bbe0-f6c093944de0",
    "_uuid": "96ee002c-c82f-4216-9e2c-fb37946c7a80",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Zip outputs for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3299685-8083-447f-b8cd-1cafd278c76f",
    "_uuid": "209452a2-1f24-48d3-92f8-671cdcb23ec9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-12T00:48:49.367313Z",
     "iopub.status.busy": "2025-10-12T00:48:49.366733Z",
     "iopub.status.idle": "2025-10-12T00:48:56.340302Z",
     "shell.execute_reply": "2025-10-12T00:48:56.339434Z",
     "shell.execute_reply.started": "2025-10-12T00:48:49.367288Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "essential_files = [\n",
    "    \"adapter_config.json\",\n",
    "    \"adapter_model.safetensors\",\n",
    "    \"added_tokens.json\",\n",
    "    \"merges.txt\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"vocab.json\",\n",
    "]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "    \n",
    "    for file_name in essential_files:\n",
    "        src_file = outdir / file_name\n",
    "        if src_file.exists():\n",
    "            shutil.copy2(src_file, temp_path / file_name)\n",
    "            print(f\"Copied: {file_name}\")\n",
    "        else:\n",
    "            print(f\"Missing: {file_name}\")\n",
    "    \n",
    "    zip_path = Path(\"/kaggle/working/qwen2.5-3b-instruct-fine-tuned.zip\")\n",
    "    shutil.make_archive(\n",
    "        str(zip_path.with_suffix(\"\")), \n",
    "        \"zip\", \n",
    "        str(temp_path)\n",
    "    )\n",
    "\n",
    "!ls -lh /kaggle/working/qwen2.5-3b-instruct-fine-tuned.zip\n",
    "!unzip -l /kaggle/working/qwen2.5-3b-instruct-fine-tuned.zip"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8435620,
     "sourceId": 13308042,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 266653060,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
